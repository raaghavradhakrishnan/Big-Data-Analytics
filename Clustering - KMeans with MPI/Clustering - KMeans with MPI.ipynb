{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file kmeans.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.sparse import vstack,csr_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.spatial import distance\n",
    "from collections import Counter\n",
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "\"\"\"Read, Preprocess, TFIDF and Vectorizing of Data\"\"\"\n",
    "def read_data():\n",
    "\n",
    "    newsgroup_train = fetch_20newsgroups(subset = \"train\", remove = ('headers','footers','quotes'))\n",
    "    text_df = pd.DataFrame([str(dat) for dat in newsgroup_train.data])\n",
    "    text_df.columns = [\"Text\"]\n",
    "    stemmer = PorterStemmer()\n",
    "    text_df['stemmed'] = text_df[\"Text\"].apply(lambda x: \" \".join([stemmer.stem(str(y)) for y in x.split()]))\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    text_vector = vectorizer.fit_transform(text_df.stemmed.dropna())\n",
    "    return text_vector,text_df\n",
    "\n",
    "def read_datacsv():\n",
    "    text_df = pd.read_csv(\"Stemmed_Main.csv\")\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    vector = vectorizer.fit_transform(text_df.stemmed.dropna())\n",
    "    return vector\n",
    "\n",
    "\"\"\"Findining random initial centroid from the sparse data\"\"\"\n",
    "def  initial_centroid(text_vector,k):\n",
    "    centroid = []\n",
    "    centroid_pos = list(np.random.randint(text_vector.shape[0],size=k))\n",
    "    centroid = text_vector[centroid_pos[0],:].todense()\n",
    "    for i in centroid_pos[1:]:\n",
    "        temp = np.array(text_vector[i,:].todense())\n",
    "        centroid = np.concatenate((centroid,temp))\n",
    "    return(centroid)\n",
    "\n",
    "\"\"\"Finding the euclidean distance between two instances\"\"\"\n",
    "def euclidean_distance(text_vector,centroid):\n",
    "    dist = np.sqrt(text_vector.dot(centroid.T))\n",
    "    cluster = np.argmin(dist, axis=1)\n",
    "    return(np.array(cluster))\n",
    "\n",
    "\n",
    "\"\"\"Finding local sum by the workers\"\"\"\n",
    "def centroid_mean(cluster,text_vector):\n",
    "    group_sum = np.array([])\n",
    "    clusters = np.unique(cluster)\n",
    "    for clust in clusters:\n",
    "        group = scipy.sparse.vstack([text_vector[j] for j in range(text_vector.shape[0]) if cluster[j] == clust])\n",
    "        group_sum = scipy.sparse.vstack((group_sum,np.array(group.sum(axis = 0))))\n",
    "    group_sum = (group_sum.tocsr())[1:,]\n",
    "    return group_sum\n",
    "\n",
    "\"\"\"Finding global mean by the master\"\"\"\n",
    "def updated_centroid(cluster,text_vector):\n",
    "    new_centroid = np.array([])\n",
    "    clusters = np.unique(cluster)\n",
    "    for clust in clusters:\n",
    "        group = scipy.sparse.vstack([text_vector[j] for j in range(text_vector.shape[0]) if cluster[j] == clust])\n",
    "        cnt = list(cluster).count(clust)\n",
    "        new_centroid = scipy.sparse.vstack((new_centroid,np.array(group.sum(axis = 0)))) / cnt\n",
    "    new_centroid = (new_centroid.tocsr())[1:,]\n",
    "    return new_centroid\n",
    "\n",
    "\n",
    "\"\"\"Initializing communicators\"\"\"\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_vector = read_datacsv()\n",
    "    k = 4\n",
    "    centroid = initial_centroid(text_vector,k)                \n",
    "\n",
    "    cluster = []\n",
    "    time_list = []\n",
    "    starttime = time.time()\n",
    "    for epoch in range(10):\n",
    "        if rank == 0:\n",
    "            for i in range(1,size):  \n",
    "                centroid_sum = np.array([])\n",
    "                old_cluster = cluster\n",
    "                rows = text_vector.shape[0]\n",
    "                start = int((i-1)*(rows/(size-1)))\n",
    "                end = int(((rows/(size-1))*i))\n",
    "                comm.send(text_vector[start:end,:],dest = i,tag = 1)\n",
    "                comm.send(centroid,dest = i,tag = 2)\n",
    "                centroid_sum = scipy.sparse.vstack((centroid_sum,comm.recv(source = i, tag = 3)))\n",
    "                cluster.extend(comm.recv(source = i,tag = 4))\n",
    "            centroid = updated_centroid(cluster,text_vector)\n",
    "            if (cluster == old_cluster).all():\n",
    "                print('Converged')\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            text_vector = comm.recv(source = 0, tag = 1)\n",
    "            centroid = comm.recv(source = 0, tag = 2)\n",
    "            cluster = euclidean_distance(text_vector,centroid)\n",
    "            centroid_sum = centroid_mean(cluster,text_vector)\n",
    "            comm.send(centroid_sum,dest = 0,tag = 3)\n",
    "            comm.send(cluster,dest = 0,tag = 4)\n",
    "    time_list.append(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 3 python kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 3 python kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [1,2,4,6,8]\n",
    "Tp = [124.37,118.68,125.22,130.57,132.07]\n",
    "Ts = 120.5\n",
    "speedup = [Ts/i for i in Tp[:2]]\n",
    "efficiency = [speed/p for speed,p in zip(speedup,P[:2])]\n",
    "print(\"Process\",\"   Tp\",\"\\t\\t\",\"Speedup\",\"\\t\",\"Efficiency\")\n",
    "for i,j,k,l in zip(P,Tp,speedup,efficiency):\n",
    "    print(i,\"\\t\",j,\"\\t\",round(k,2),\"\\t\\t\",round(l,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Number of Processes\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.title(\"Linear Speedup\")\n",
    "plt.plot(P[:2],speedup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [1,2,4,6,8]\n",
    "Tp = [124.37,118.68,125.22,130.57,132.07]\n",
    "Ts = 120.5\n",
    "speedup = [Ts/i for i in Tp]\n",
    "efficiency = [speed/p for speed,p in zip(speedup,P)]\n",
    "print(\"Process\",\"   Tp\",\"\\t\\t\",\"Speedup\",\"\\t\",\"Efficiency\")\n",
    "for i,j,k,l in zip(P,Tp,speedup,efficiency):\n",
    "    print(i,\"\\t\",j,\"\\t\",round(k,2),\"\\t\\t\",round(l,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(P,speedup)\n",
    "plt.xlabel(\"Number of Processes\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.title(\"Sub-linear Speedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
